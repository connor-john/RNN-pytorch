{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Implmentation for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/shakespeare.txt', 'r', encoding = 'utf8') as t:\n",
    "    text = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_char = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder = dict(enumerate(all_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = {char: i for i,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
       "       35, 35, 35, 35, 35, 16, 81, 35, 35, 73, 59, 31,  0, 35, 33,  1, 54,\n",
       "       59, 28, 21, 58, 35, 71, 59, 28,  1, 58, 72, 59, 28, 21, 35, 17, 28,\n",
       "       35, 27, 28, 21, 54, 59, 28, 35, 54, 44, 71, 59, 28,  1, 21, 28, 47,\n",
       "       81, 35, 35, 11,  2,  1, 58, 35, 58,  2, 28, 59, 28, 74, 60, 35, 74,\n",
       "       28,  1, 72, 58, 60, 83, 21, 35, 59, 31, 21, 28, 35,  0, 54])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def one_hot_enc(batch_text, uni_chars):\n",
    "    one_hot = np.zeros((batch_text.size, uni_chars))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), batch_text.flatten()] = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*batch_text.shape, uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "x = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_enc(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batches for training\n",
    "def gen_batch(en_text, sample_size = 10, seq_len = 50):\n",
    "    \n",
    "    char_len = sample_size * seq_len\n",
    "    num_batches = int(len(en_text) / char_len)\n",
    "    \n",
    "    en_text = en_text[: num_batches * char_len]\n",
    "    en_text = en_text.reshape((sample_size, -1))\n",
    "    \n",
    "    for n in range(0,en_text.shape[-1], seq_len):\n",
    "        x = en_text[:, n : n + seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, : -1] = x[:, 1:]\n",
    "            y[:, -1] = en_text[:, n + seq_len]\n",
    "        \n",
    "        except:\n",
    "            y[:, : -1] = x[:, 1:]\n",
    "            y[:, -1] = en_text[:, 0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "sample_text = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = gen_batch(sample_text, sample_size = 2, seq_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden = 256, num_layers = 4, drop_prob = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.all_chars = all_chars\n",
    "        \n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: i for i, char in decoder.items()}\n",
    "        \n",
    "        # Architecture\n",
    "        self.lstm = nn.LSTM(len(all_chars), num_hidden, num_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        drop_out = drop_out.contiguous().view(-1, self.num_hidden)\n",
    "        output = self.fc1(drop_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                  torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(all_chars = all_char, num_hidden = 512, num_layers = 3, drop_prob = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    params.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(params)\n",
    "# have some of params roughly equal to size of text data set to prevent over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "lr = 0.001\n",
    "train_per = 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_ind = int(len(encoded_text) * (train_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "test_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
       "       35, 35, 35, 35, 35, 16, 81, 35, 35, 73, 59, 31,  0, 35, 33,  1, 54,\n",
       "       59, 28, 21, 58, 35, 71, 59, 28,  1, 58, 72, 59, 28, 21, 35, 17])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "t = 0\n",
    "num_char = max(encoded_text) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 25 | val loss 3.2006781101226807\n",
      "epoch: 0 | step: 50 | val loss 3.1942505836486816\n",
      "epoch: 0 | step: 75 | val loss 3.196668863296509\n",
      "epoch: 0 | step: 100 | val loss 3.1933629512786865\n",
      "epoch: 0 | step: 125 | val loss 3.163191318511963\n",
      "epoch: 0 | step: 150 | val loss 3.0347700119018555\n",
      "epoch: 0 | step: 175 | val loss 2.920569896697998\n",
      "epoch: 0 | step: 200 | val loss 2.7438316345214844\n",
      "epoch: 0 | step: 225 | val loss 2.5890860557556152\n",
      "epoch: 0 | step: 250 | val loss 2.4908289909362793\n",
      "epoch: 0 | step: 275 | val loss 2.401627779006958\n",
      "epoch: 0 | step: 300 | val loss 2.3215043544769287\n",
      "epoch: 0 | step: 325 | val loss 2.2386534214019775\n",
      "epoch: 0 | step: 350 | val loss 2.181492567062378\n",
      "epoch: 0 | step: 375 | val loss 2.1380929946899414\n",
      "epoch: 0 | step: 400 | val loss 2.0923383235931396\n",
      "epoch: 0 | step: 425 | val loss 2.055506467819214\n",
      "epoch: 0 | step: 450 | val loss 2.0319364070892334\n",
      "epoch: 0 | step: 475 | val loss 1.994659185409546\n",
      "epoch: 1 | step: 500 | val loss 1.9593356847763062\n",
      "epoch: 1 | step: 525 | val loss 1.941281795501709\n",
      "epoch: 1 | step: 550 | val loss 1.912969708442688\n",
      "epoch: 1 | step: 575 | val loss 1.8807638883590698\n",
      "epoch: 1 | step: 600 | val loss 1.856329321861267\n",
      "epoch: 1 | step: 625 | val loss 1.8353757858276367\n",
      "epoch: 1 | step: 650 | val loss 1.8177578449249268\n",
      "epoch: 1 | step: 675 | val loss 1.7982470989227295\n",
      "epoch: 1 | step: 700 | val loss 1.776885747909546\n",
      "epoch: 1 | step: 725 | val loss 1.7596570253372192\n",
      "epoch: 1 | step: 750 | val loss 1.744950532913208\n",
      "epoch: 1 | step: 775 | val loss 1.7337353229522705\n",
      "epoch: 1 | step: 800 | val loss 1.720521092414856\n",
      "epoch: 1 | step: 825 | val loss 1.708575963973999\n",
      "epoch: 1 | step: 850 | val loss 1.6933187246322632\n",
      "epoch: 1 | step: 875 | val loss 1.6835534572601318\n",
      "epoch: 1 | step: 900 | val loss 1.6739599704742432\n",
      "epoch: 1 | step: 925 | val loss 1.6583821773529053\n",
      "epoch: 1 | step: 950 | val loss 1.6478779315948486\n",
      "epoch: 1 | step: 975 | val loss 1.6332632303237915\n",
      "epoch: 2 | step: 1000 | val loss 1.6256550550460815\n",
      "epoch: 2 | step: 1025 | val loss 1.6292904615402222\n",
      "epoch: 2 | step: 1050 | val loss 1.610968828201294\n",
      "epoch: 2 | step: 1075 | val loss 1.602009892463684\n",
      "epoch: 2 | step: 1100 | val loss 1.5918368101119995\n",
      "epoch: 2 | step: 1125 | val loss 1.58465576171875\n",
      "epoch: 2 | step: 1150 | val loss 1.5767920017242432\n",
      "epoch: 2 | step: 1175 | val loss 1.5699386596679688\n",
      "epoch: 2 | step: 1200 | val loss 1.560853123664856\n",
      "epoch: 2 | step: 1225 | val loss 1.5525126457214355\n",
      "epoch: 2 | step: 1250 | val loss 1.545474886894226\n",
      "epoch: 2 | step: 1275 | val loss 1.5441490411758423\n",
      "epoch: 2 | step: 1300 | val loss 1.5366439819335938\n",
      "epoch: 2 | step: 1325 | val loss 1.53057861328125\n",
      "epoch: 2 | step: 1350 | val loss 1.5267679691314697\n",
      "epoch: 2 | step: 1375 | val loss 1.5219146013259888\n",
      "epoch: 2 | step: 1400 | val loss 1.518776774406433\n",
      "epoch: 2 | step: 1425 | val loss 1.5184862613677979\n",
      "epoch: 2 | step: 1450 | val loss 1.5040814876556396\n",
      "epoch: 3 | step: 1475 | val loss 1.5013999938964844\n",
      "epoch: 3 | step: 1500 | val loss 1.4970200061798096\n",
      "epoch: 3 | step: 1525 | val loss 1.4951995611190796\n",
      "epoch: 3 | step: 1550 | val loss 1.4907264709472656\n",
      "epoch: 3 | step: 1575 | val loss 1.4879941940307617\n",
      "epoch: 3 | step: 1600 | val loss 1.4795304536819458\n",
      "epoch: 3 | step: 1625 | val loss 1.4773075580596924\n",
      "epoch: 3 | step: 1650 | val loss 1.4711743593215942\n",
      "epoch: 3 | step: 1675 | val loss 1.4689478874206543\n",
      "epoch: 3 | step: 1700 | val loss 1.4608525037765503\n",
      "epoch: 3 | step: 1725 | val loss 1.4582866430282593\n",
      "epoch: 3 | step: 1750 | val loss 1.4556125402450562\n",
      "epoch: 3 | step: 1775 | val loss 1.450096607208252\n",
      "epoch: 3 | step: 1800 | val loss 1.4505587816238403\n",
      "epoch: 3 | step: 1825 | val loss 1.4548672437667847\n",
      "epoch: 3 | step: 1850 | val loss 1.4538906812667847\n",
      "epoch: 3 | step: 1875 | val loss 1.4478189945220947\n",
      "epoch: 3 | step: 1900 | val loss 1.4470487833023071\n",
      "epoch: 3 | step: 1925 | val loss 1.443721890449524\n",
      "epoch: 3 | step: 1950 | val loss 1.436825156211853\n",
      "epoch: 4 | step: 1975 | val loss 1.4403057098388672\n",
      "epoch: 4 | step: 2000 | val loss 1.4366382360458374\n",
      "epoch: 4 | step: 2025 | val loss 1.4317651987075806\n",
      "epoch: 4 | step: 2050 | val loss 1.4282869100570679\n",
      "epoch: 4 | step: 2075 | val loss 1.4293588399887085\n",
      "epoch: 4 | step: 2100 | val loss 1.426162838935852\n",
      "epoch: 4 | step: 2125 | val loss 1.4195001125335693\n",
      "epoch: 4 | step: 2150 | val loss 1.4170045852661133\n",
      "epoch: 4 | step: 2175 | val loss 1.4132159948349\n",
      "epoch: 4 | step: 2200 | val loss 1.4127106666564941\n",
      "epoch: 4 | step: 2225 | val loss 1.4107598066329956\n",
      "epoch: 4 | step: 2250 | val loss 1.4050114154815674\n",
      "epoch: 4 | step: 2275 | val loss 1.4074811935424805\n",
      "epoch: 4 | step: 2300 | val loss 1.4061604738235474\n",
      "epoch: 4 | step: 2325 | val loss 1.4110503196716309\n",
      "epoch: 4 | step: 2350 | val loss 1.408478856086731\n",
      "epoch: 4 | step: 2375 | val loss 1.4047266244888306\n",
      "epoch: 4 | step: 2400 | val loss 1.4042631387710571\n",
      "epoch: 4 | step: 2425 | val loss 1.4041320085525513\n",
      "epoch: 4 | step: 2450 | val loss 1.3971554040908813\n",
      "epoch: 5 | step: 2475 | val loss 1.4016631841659546\n",
      "epoch: 5 | step: 2500 | val loss 1.3978700637817383\n",
      "epoch: 5 | step: 2525 | val loss 1.3941657543182373\n",
      "epoch: 5 | step: 2550 | val loss 1.393441081047058\n",
      "epoch: 5 | step: 2575 | val loss 1.3894211053848267\n",
      "epoch: 5 | step: 2600 | val loss 1.3916385173797607\n",
      "epoch: 5 | step: 2625 | val loss 1.3914800882339478\n",
      "epoch: 5 | step: 2650 | val loss 1.3860702514648438\n",
      "epoch: 5 | step: 2675 | val loss 1.3809970617294312\n",
      "epoch: 5 | step: 2700 | val loss 1.377301573753357\n",
      "epoch: 5 | step: 2725 | val loss 1.3850892782211304\n",
      "epoch: 5 | step: 2750 | val loss 1.38054621219635\n",
      "epoch: 5 | step: 2775 | val loss 1.3772810697555542\n",
      "epoch: 5 | step: 2800 | val loss 1.3851797580718994\n",
      "epoch: 5 | step: 2825 | val loss 1.3822855949401855\n",
      "epoch: 5 | step: 2850 | val loss 1.380574107170105\n",
      "epoch: 5 | step: 2875 | val loss 1.3798904418945312\n",
      "epoch: 5 | step: 2900 | val loss 1.3742618560791016\n",
      "epoch: 5 | step: 2925 | val loss 1.3778148889541626\n",
      "epoch: 6 | step: 2950 | val loss 1.3755158185958862\n",
      "epoch: 6 | step: 2975 | val loss 1.3786367177963257\n",
      "epoch: 6 | step: 3000 | val loss 1.3737995624542236\n",
      "epoch: 6 | step: 3025 | val loss 1.3698116540908813\n",
      "epoch: 6 | step: 3050 | val loss 1.3716853857040405\n",
      "epoch: 6 | step: 3075 | val loss 1.3629693984985352\n",
      "epoch: 6 | step: 3100 | val loss 1.3676406145095825\n",
      "epoch: 6 | step: 3125 | val loss 1.362030029296875\n",
      "epoch: 6 | step: 3150 | val loss 1.363118052482605\n",
      "epoch: 6 | step: 3175 | val loss 1.3670464754104614\n",
      "epoch: 6 | step: 3200 | val loss 1.359101414680481\n",
      "epoch: 6 | step: 3225 | val loss 1.3616372346878052\n",
      "epoch: 6 | step: 3250 | val loss 1.3580889701843262\n",
      "epoch: 6 | step: 3275 | val loss 1.35874342918396\n",
      "epoch: 6 | step: 3300 | val loss 1.3616775274276733\n",
      "epoch: 6 | step: 3325 | val loss 1.3672412633895874\n",
      "epoch: 6 | step: 3350 | val loss 1.3660188913345337\n",
      "epoch: 6 | step: 3375 | val loss 1.3622474670410156\n",
      "epoch: 6 | step: 3400 | val loss 1.359877586364746\n",
      "epoch: 6 | step: 3425 | val loss 1.3589786291122437\n",
      "epoch: 7 | step: 3450 | val loss 1.3599543571472168\n",
      "epoch: 7 | step: 3475 | val loss 1.357916235923767\n",
      "epoch: 7 | step: 3500 | val loss 1.3589632511138916\n",
      "epoch: 7 | step: 3525 | val loss 1.3544936180114746\n",
      "epoch: 7 | step: 3550 | val loss 1.3543481826782227\n",
      "epoch: 7 | step: 3575 | val loss 1.3547743558883667\n",
      "epoch: 7 | step: 3600 | val loss 1.3521569967269897\n",
      "epoch: 7 | step: 3625 | val loss 1.3470396995544434\n",
      "epoch: 7 | step: 3650 | val loss 1.3459491729736328\n",
      "epoch: 7 | step: 3675 | val loss 1.349059820175171\n",
      "epoch: 7 | step: 3700 | val loss 1.3485313653945923\n",
      "epoch: 7 | step: 3725 | val loss 1.3478337526321411\n",
      "epoch: 7 | step: 3750 | val loss 1.344283938407898\n",
      "epoch: 7 | step: 3775 | val loss 1.3457856178283691\n",
      "epoch: 7 | step: 3800 | val loss 1.3489948511123657\n",
      "epoch: 7 | step: 3825 | val loss 1.347771406173706\n",
      "epoch: 7 | step: 3850 | val loss 1.3498204946517944\n",
      "epoch: 7 | step: 3875 | val loss 1.343658685684204\n",
      "epoch: 7 | step: 3900 | val loss 1.345628261566162\n",
      "epoch: 8 | step: 3925 | val loss 1.345704197883606\n",
      "epoch: 8 | step: 3950 | val loss 1.3477914333343506\n",
      "epoch: 8 | step: 3975 | val loss 1.343644380569458\n",
      "epoch: 8 | step: 4000 | val loss 1.344093680381775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | step: 4025 | val loss 1.3442476987838745\n",
      "epoch: 8 | step: 4050 | val loss 1.342037320137024\n",
      "epoch: 8 | step: 4075 | val loss 1.3453288078308105\n",
      "epoch: 8 | step: 4100 | val loss 1.3479368686676025\n",
      "epoch: 8 | step: 4125 | val loss 1.3404797315597534\n",
      "epoch: 8 | step: 4150 | val loss 1.3338595628738403\n",
      "epoch: 8 | step: 4175 | val loss 1.3303786516189575\n",
      "epoch: 8 | step: 4200 | val loss 1.3415168523788452\n",
      "epoch: 8 | step: 4225 | val loss 1.333423376083374\n",
      "epoch: 8 | step: 4250 | val loss 1.3387411832809448\n",
      "epoch: 8 | step: 4275 | val loss 1.3405542373657227\n",
      "epoch: 8 | step: 4300 | val loss 1.339881181716919\n",
      "epoch: 8 | step: 4325 | val loss 1.3433884382247925\n",
      "epoch: 8 | step: 4350 | val loss 1.3436346054077148\n",
      "epoch: 8 | step: 4375 | val loss 1.3380862474441528\n",
      "epoch: 8 | step: 4400 | val loss 1.336854100227356\n",
      "epoch: 9 | step: 4425 | val loss 1.3353625535964966\n",
      "epoch: 9 | step: 4450 | val loss 1.3311353921890259\n",
      "epoch: 9 | step: 4475 | val loss 1.3346662521362305\n",
      "epoch: 9 | step: 4500 | val loss 1.3336451053619385\n",
      "epoch: 9 | step: 4525 | val loss 1.3356319665908813\n",
      "epoch: 9 | step: 4550 | val loss 1.3338528871536255\n",
      "epoch: 9 | step: 4575 | val loss 1.338518500328064\n",
      "epoch: 9 | step: 4600 | val loss 1.3386541604995728\n",
      "epoch: 9 | step: 4625 | val loss 1.3330128192901611\n",
      "epoch: 9 | step: 4650 | val loss 1.3304011821746826\n",
      "epoch: 9 | step: 4675 | val loss 1.3280291557312012\n",
      "epoch: 9 | step: 4700 | val loss 1.3321417570114136\n",
      "epoch: 9 | step: 4725 | val loss 1.3299071788787842\n",
      "epoch: 9 | step: 4750 | val loss 1.3293508291244507\n",
      "epoch: 9 | step: 4775 | val loss 1.3344647884368896\n",
      "epoch: 9 | step: 4800 | val loss 1.3329957723617554\n",
      "epoch: 9 | step: 4825 | val loss 1.337888240814209\n",
      "epoch: 9 | step: 4850 | val loss 1.329996943473816\n",
      "epoch: 9 | step: 4875 | val loss 1.33554208278656\n",
      "epoch: 9 | step: 4900 | val loss 1.332424283027649\n",
      "epoch: 10 | step: 4925 | val loss 1.3298869132995605\n",
      "epoch: 10 | step: 4950 | val loss 1.332011103630066\n",
      "epoch: 10 | step: 4975 | val loss 1.328755259513855\n",
      "epoch: 10 | step: 5000 | val loss 1.327723741531372\n",
      "epoch: 10 | step: 5025 | val loss 1.3297545909881592\n",
      "epoch: 10 | step: 5050 | val loss 1.3265212774276733\n",
      "epoch: 10 | step: 5075 | val loss 1.3311901092529297\n",
      "epoch: 10 | step: 5100 | val loss 1.328201174736023\n",
      "epoch: 10 | step: 5125 | val loss 1.3250861167907715\n",
      "epoch: 10 | step: 5150 | val loss 1.318860650062561\n",
      "epoch: 10 | step: 5175 | val loss 1.3270844221115112\n",
      "epoch: 10 | step: 5200 | val loss 1.3280781507492065\n",
      "epoch: 10 | step: 5225 | val loss 1.3282092809677124\n",
      "epoch: 10 | step: 5250 | val loss 1.3281002044677734\n",
      "epoch: 10 | step: 5275 | val loss 1.3274271488189697\n",
      "epoch: 10 | step: 5300 | val loss 1.33024263381958\n",
      "epoch: 10 | step: 5325 | val loss 1.332929015159607\n",
      "epoch: 10 | step: 5350 | val loss 1.3247190713882446\n",
      "epoch: 10 | step: 5375 | val loss 1.3262498378753662\n",
      "epoch: 11 | step: 5400 | val loss 1.3255796432495117\n",
      "epoch: 11 | step: 5425 | val loss 1.323791265487671\n",
      "epoch: 11 | step: 5450 | val loss 1.3204845190048218\n",
      "epoch: 11 | step: 5475 | val loss 1.3201674222946167\n",
      "epoch: 11 | step: 5500 | val loss 1.3221102952957153\n",
      "epoch: 11 | step: 5525 | val loss 1.3193649053573608\n",
      "epoch: 11 | step: 5550 | val loss 1.322542428970337\n",
      "epoch: 11 | step: 5575 | val loss 1.3251454830169678\n",
      "epoch: 11 | step: 5600 | val loss 1.3216954469680786\n",
      "epoch: 11 | step: 5625 | val loss 1.3232731819152832\n",
      "epoch: 11 | step: 5650 | val loss 1.3167963027954102\n",
      "epoch: 11 | step: 5675 | val loss 1.3196483850479126\n",
      "epoch: 11 | step: 5700 | val loss 1.316393256187439\n",
      "epoch: 11 | step: 5725 | val loss 1.3173125982284546\n",
      "epoch: 11 | step: 5750 | val loss 1.323436975479126\n",
      "epoch: 11 | step: 5775 | val loss 1.3254468441009521\n",
      "epoch: 11 | step: 5800 | val loss 1.3302069902420044\n",
      "epoch: 11 | step: 5825 | val loss 1.327817440032959\n",
      "epoch: 11 | step: 5850 | val loss 1.327101707458496\n",
      "epoch: 11 | step: 5875 | val loss 1.3243354558944702\n",
      "epoch: 12 | step: 5900 | val loss 1.3257569074630737\n",
      "epoch: 12 | step: 5925 | val loss 1.3257615566253662\n",
      "epoch: 12 | step: 5950 | val loss 1.328495979309082\n",
      "epoch: 12 | step: 5975 | val loss 1.3219428062438965\n",
      "epoch: 12 | step: 6000 | val loss 1.3221937417984009\n",
      "epoch: 12 | step: 6025 | val loss 1.3244482278823853\n",
      "epoch: 12 | step: 6050 | val loss 1.3234974145889282\n",
      "epoch: 12 | step: 6075 | val loss 1.3216874599456787\n",
      "epoch: 12 | step: 6100 | val loss 1.319947361946106\n",
      "epoch: 12 | step: 6125 | val loss 1.319198489189148\n",
      "epoch: 12 | step: 6150 | val loss 1.3213914632797241\n",
      "epoch: 12 | step: 6175 | val loss 1.3150644302368164\n",
      "epoch: 12 | step: 6200 | val loss 1.3141891956329346\n",
      "epoch: 12 | step: 6225 | val loss 1.317569375038147\n",
      "epoch: 12 | step: 6250 | val loss 1.3183380365371704\n",
      "epoch: 12 | step: 6275 | val loss 1.3180004358291626\n",
      "epoch: 12 | step: 6300 | val loss 1.3242852687835693\n",
      "epoch: 12 | step: 6325 | val loss 1.320969581604004\n",
      "epoch: 12 | step: 6350 | val loss 1.3226287364959717\n",
      "epoch: 13 | step: 6375 | val loss 1.3245576620101929\n",
      "epoch: 13 | step: 6400 | val loss 1.3278356790542603\n",
      "epoch: 13 | step: 6425 | val loss 1.3204126358032227\n",
      "epoch: 13 | step: 6450 | val loss 1.3221104145050049\n",
      "epoch: 13 | step: 6475 | val loss 1.3187414407730103\n",
      "epoch: 13 | step: 6500 | val loss 1.314825177192688\n",
      "epoch: 13 | step: 6525 | val loss 1.3196539878845215\n",
      "epoch: 13 | step: 6550 | val loss 1.3189064264297485\n",
      "epoch: 13 | step: 6575 | val loss 1.3188146352767944\n",
      "epoch: 13 | step: 6600 | val loss 1.3137235641479492\n",
      "epoch: 13 | step: 6625 | val loss 1.31549072265625\n",
      "epoch: 13 | step: 6650 | val loss 1.3197287321090698\n",
      "epoch: 13 | step: 6675 | val loss 1.3146435022354126\n",
      "epoch: 13 | step: 6700 | val loss 1.3112257719039917\n",
      "epoch: 13 | step: 6725 | val loss 1.3215843439102173\n",
      "epoch: 13 | step: 6750 | val loss 1.321139931678772\n",
      "epoch: 13 | step: 6775 | val loss 1.3196635246276855\n",
      "epoch: 13 | step: 6800 | val loss 1.322135329246521\n",
      "epoch: 13 | step: 6825 | val loss 1.3188884258270264\n",
      "epoch: 13 | step: 6850 | val loss 1.3188611268997192\n",
      "epoch: 14 | step: 6875 | val loss 1.3176617622375488\n",
      "epoch: 14 | step: 6900 | val loss 1.3197343349456787\n",
      "epoch: 14 | step: 6925 | val loss 1.3170887231826782\n",
      "epoch: 14 | step: 6950 | val loss 1.310971736907959\n",
      "epoch: 14 | step: 6975 | val loss 1.3154184818267822\n",
      "epoch: 14 | step: 7000 | val loss 1.3148077726364136\n",
      "epoch: 14 | step: 7025 | val loss 1.32038414478302\n",
      "epoch: 14 | step: 7050 | val loss 1.3189382553100586\n",
      "epoch: 14 | step: 7075 | val loss 1.3155308961868286\n",
      "epoch: 14 | step: 7100 | val loss 1.3119807243347168\n",
      "epoch: 14 | step: 7125 | val loss 1.3125170469284058\n",
      "epoch: 14 | step: 7150 | val loss 1.318113923072815\n",
      "epoch: 14 | step: 7175 | val loss 1.3105013370513916\n",
      "epoch: 14 | step: 7200 | val loss 1.3154398202896118\n",
      "epoch: 14 | step: 7225 | val loss 1.320610761642456\n",
      "epoch: 14 | step: 7250 | val loss 1.3150995969772339\n",
      "epoch: 14 | step: 7275 | val loss 1.3161014318466187\n",
      "epoch: 14 | step: 7300 | val loss 1.3148696422576904\n",
      "epoch: 14 | step: 7325 | val loss 1.3158612251281738\n",
      "epoch: 14 | step: 7350 | val loss 1.3138633966445923\n",
      "epoch: 15 | step: 7375 | val loss 1.315542459487915\n",
      "epoch: 15 | step: 7400 | val loss 1.3164554834365845\n",
      "epoch: 15 | step: 7425 | val loss 1.3161990642547607\n",
      "epoch: 15 | step: 7450 | val loss 1.3175387382507324\n",
      "epoch: 15 | step: 7475 | val loss 1.3143548965454102\n",
      "epoch: 15 | step: 7500 | val loss 1.3120441436767578\n",
      "epoch: 15 | step: 7525 | val loss 1.3148326873779297\n",
      "epoch: 15 | step: 7550 | val loss 1.3152368068695068\n",
      "epoch: 15 | step: 7575 | val loss 1.3175337314605713\n",
      "epoch: 15 | step: 7600 | val loss 1.3118276596069336\n",
      "epoch: 15 | step: 7625 | val loss 1.3143657445907593\n",
      "epoch: 15 | step: 7650 | val loss 1.3125802278518677\n",
      "epoch: 15 | step: 7675 | val loss 1.309570550918579\n",
      "epoch: 15 | step: 7700 | val loss 1.314908742904663\n",
      "epoch: 15 | step: 7725 | val loss 1.3153846263885498\n",
      "epoch: 15 | step: 7750 | val loss 1.3151872158050537\n",
      "epoch: 15 | step: 7775 | val loss 1.3188170194625854\n",
      "epoch: 15 | step: 7800 | val loss 1.3117191791534424\n",
      "epoch: 15 | step: 7825 | val loss 1.3133610486984253\n",
      "epoch: 16 | step: 7850 | val loss 1.3156815767288208\n",
      "epoch: 16 | step: 7875 | val loss 1.312012791633606\n",
      "epoch: 16 | step: 7900 | val loss 1.3172545433044434\n",
      "epoch: 16 | step: 7925 | val loss 1.3141393661499023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | step: 7950 | val loss 1.3134551048278809\n",
      "epoch: 16 | step: 7975 | val loss 1.3046443462371826\n",
      "epoch: 16 | step: 8000 | val loss 1.3109959363937378\n",
      "epoch: 16 | step: 8025 | val loss 1.3143054246902466\n",
      "epoch: 16 | step: 8050 | val loss 1.315353512763977\n",
      "epoch: 16 | step: 8075 | val loss 1.3089423179626465\n",
      "epoch: 16 | step: 8100 | val loss 1.3067079782485962\n",
      "epoch: 16 | step: 8125 | val loss 1.3147329092025757\n",
      "epoch: 16 | step: 8150 | val loss 1.3084266185760498\n",
      "epoch: 16 | step: 8175 | val loss 1.3107188940048218\n",
      "epoch: 16 | step: 8200 | val loss 1.314167857170105\n",
      "epoch: 16 | step: 8225 | val loss 1.3164421319961548\n",
      "epoch: 16 | step: 8250 | val loss 1.3191379308700562\n",
      "epoch: 16 | step: 8275 | val loss 1.3137906789779663\n",
      "epoch: 16 | step: 8300 | val loss 1.3177214860916138\n",
      "epoch: 16 | step: 8325 | val loss 1.3197646141052246\n",
      "epoch: 17 | step: 8350 | val loss 1.315169095993042\n",
      "epoch: 17 | step: 8375 | val loss 1.3153284788131714\n",
      "epoch: 17 | step: 8400 | val loss 1.3186155557632446\n",
      "epoch: 17 | step: 8425 | val loss 1.309205412864685\n",
      "epoch: 17 | step: 8450 | val loss 1.3145623207092285\n",
      "epoch: 17 | step: 8475 | val loss 1.3101669549942017\n",
      "epoch: 17 | step: 8500 | val loss 1.3119395971298218\n",
      "epoch: 17 | step: 8525 | val loss 1.307867407798767\n",
      "epoch: 17 | step: 8550 | val loss 1.311769962310791\n",
      "epoch: 17 | step: 8575 | val loss 1.3075779676437378\n",
      "epoch: 17 | step: 8600 | val loss 1.312833547592163\n",
      "epoch: 17 | step: 8625 | val loss 1.313687801361084\n",
      "epoch: 17 | step: 8650 | val loss 1.3083633184432983\n",
      "epoch: 17 | step: 8675 | val loss 1.315751314163208\n",
      "epoch: 17 | step: 8700 | val loss 1.3218591213226318\n",
      "epoch: 17 | step: 8725 | val loss 1.3201849460601807\n",
      "epoch: 17 | step: 8750 | val loss 1.3153613805770874\n",
      "epoch: 17 | step: 8775 | val loss 1.3149843215942383\n",
      "epoch: 17 | step: 8800 | val loss 1.3161115646362305\n",
      "epoch: 18 | step: 8825 | val loss 1.3140281438827515\n",
      "epoch: 18 | step: 8850 | val loss 1.3118575811386108\n",
      "epoch: 18 | step: 8875 | val loss 1.3108391761779785\n",
      "epoch: 18 | step: 8900 | val loss 1.3163079023361206\n",
      "epoch: 18 | step: 8925 | val loss 1.315077781677246\n",
      "epoch: 18 | step: 8950 | val loss 1.3135952949523926\n",
      "epoch: 18 | step: 8975 | val loss 1.3125337362289429\n",
      "epoch: 18 | step: 9000 | val loss 1.3143584728240967\n",
      "epoch: 18 | step: 9025 | val loss 1.3083032369613647\n",
      "epoch: 18 | step: 9050 | val loss 1.3043208122253418\n",
      "epoch: 18 | step: 9075 | val loss 1.303225040435791\n",
      "epoch: 18 | step: 9100 | val loss 1.308997631072998\n",
      "epoch: 18 | step: 9125 | val loss 1.3061778545379639\n",
      "epoch: 18 | step: 9150 | val loss 1.3098679780960083\n",
      "epoch: 18 | step: 9175 | val loss 1.3107556104660034\n",
      "epoch: 18 | step: 9200 | val loss 1.3116785287857056\n",
      "epoch: 18 | step: 9225 | val loss 1.3133326768875122\n",
      "epoch: 18 | step: 9250 | val loss 1.3130993843078613\n",
      "epoch: 18 | step: 9275 | val loss 1.311859130859375\n",
      "epoch: 18 | step: 9300 | val loss 1.3135322332382202\n",
      "epoch: 19 | step: 9325 | val loss 1.310671329498291\n",
      "epoch: 19 | step: 9350 | val loss 1.3099322319030762\n",
      "epoch: 19 | step: 9375 | val loss 1.3065922260284424\n",
      "epoch: 19 | step: 9400 | val loss 1.3070989847183228\n",
      "epoch: 19 | step: 9425 | val loss 1.3141547441482544\n",
      "epoch: 19 | step: 9450 | val loss 1.3117421865463257\n",
      "epoch: 19 | step: 9475 | val loss 1.3113601207733154\n",
      "epoch: 19 | step: 9500 | val loss 1.311583161354065\n",
      "epoch: 19 | step: 9525 | val loss 1.3059446811676025\n",
      "epoch: 19 | step: 9550 | val loss 1.3041208982467651\n",
      "epoch: 19 | step: 9575 | val loss 1.3035337924957275\n",
      "epoch: 19 | step: 9600 | val loss 1.3085311651229858\n",
      "epoch: 19 | step: 9625 | val loss 1.3043925762176514\n",
      "epoch: 19 | step: 9650 | val loss 1.3083877563476562\n",
      "epoch: 19 | step: 9675 | val loss 1.3120814561843872\n",
      "epoch: 19 | step: 9700 | val loss 1.3164491653442383\n",
      "epoch: 19 | step: 9725 | val loss 1.3157563209533691\n",
      "epoch: 19 | step: 9750 | val loss 1.3071529865264893\n",
      "epoch: 19 | step: 9775 | val loss 1.3099496364593506\n",
      "epoch: 19 | step: 9800 | val loss 1.3138666152954102\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in gen_batch(train_data, batch_size, seq_len):\n",
    "        \n",
    "        t += 1\n",
    "        x = one_hot_enc(x, num_char)\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_out, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_out, targets.view(batch_size * seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Avoid exploding gradient\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # validation step\n",
    "        if t % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in gen_batch(test_data, batch_size, seq_len):\n",
    "                \n",
    "                x = one_hot_enc(x,num_char)\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                    \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_out, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_out, targets.view(batch_size * seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training \n",
    "            model.train()\n",
    "            \n",
    "            print(f\"epoch: {i} | step: {t} | val loss {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converges around a 1.30 loss\n",
    "# premise: loss does not reduce after a whole epoch\n",
    "# save model\n",
    "name = 'CharRNN_hidden512_layers3_shakes.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction \n",
    "def predict_next(model, char, hidden = None, k = 1):\n",
    "    \n",
    "    encoded_text = model.encoder[char]\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    encoded_text = one_hot_enc(encoded_text, len(model.all_chars))\n",
    "    \n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    inputs = inputs.cuda()\n",
    "    \n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "    \n",
    "    probs = F.softmax(lstm_out, dim = 1).data\n",
    "    probs = probs.cpu()\n",
    "    \n",
    "    probs, index_pos = probs.topk(k)\n",
    "    index_pos = index_pos.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    probs = probs/probs.sum()\n",
    "    \n",
    "    char = np.random.choice(index_pos, p = probs)\n",
    "    \n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, size, seed = 'The', k = 1):\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    output_chars = [c for c in seed]\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    for char in seed:\n",
    "        char, hidden = predict_next(model, char, hidden, k = k)\n",
    "    \n",
    "    output_chars.append(char)\n",
    "    \n",
    "    for i in range(size):\n",
    "        char, hidden = predict_next(model, output_chars[-1], hidden, k = k)\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "# model.load_state_dict(torch.load(name))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PRINCE HERRY and MARCIUS and SOMERSET\n",
      "  SHYLOCK. It is no more to strike me, and the warding\n",
      "    To this, I shall see, that I have set him,\n",
      "    And to the world was stay with him on this,\n",
      "    With sorrow that I would have seen thy state,\n",
      "    With the service of merry trial and\n",
      "    That the subject of this body were\n",
      "    When the securest thing of his tongue does.\n",
      "    The sun are strange and too and this doth shake\n",
      "    The way and blessed with a field of heaven\n",
      "    As that the world will stay again.\n",
      "                                             Exit, worse than her horse.  \n",
      "  CORIOLANUS. The way, a service, thanks, and see how she hath.\n",
      "    I have sent to me.\n",
      "  CLEOPATRA. Welcome, my lady,\n",
      "    And that thou wast not so made and set thee.  \n",
      "    I am an enemy, and we say, they say,\n",
      "    That whom thy sons, while he shall strong and strange\n",
      "    To stand to seek the company and strong,\n",
      "    The market or a false of the conscience,\n",
      "    Whose truth and hand of mine are striking thine.\n",
      "    Though I\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(generate_text(model, 1000, seed = 'The ', k = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
