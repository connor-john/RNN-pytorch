{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Implmentation for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/shakespeare.txt', 'r', encoding = 'utf8') as t:\n",
    "    text = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_char = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder = dict(enumerate(all_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = {char: i for i,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7, 22,  0,  7,  7, 14, 68,  3, 31,  7, 38, 67, 28,\n",
       "       68, 64, 37, 83,  7, 32, 68, 64, 67, 83, 55, 68, 64, 37,  7,  9, 64,\n",
       "        7, 10, 64, 37, 28, 68, 64,  7, 28, 69, 32, 68, 64, 67, 37, 64, 70,\n",
       "        0,  7,  7, 82, 18, 67, 83,  7, 83, 18, 64, 68, 64, 19, 60,  7, 19,\n",
       "       64, 67, 55, 83, 60, 43, 37,  7, 68,  3, 37, 64,  7, 31, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def one_hot_enc(batch_text, uni_chars):\n",
    "    one_hot = np.zeros((batch_text.size, uni_chars))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), batch_text.flatten()] = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*batch_text.shape, uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "x = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_enc(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batches for training\n",
    "def gen_batch(en_text, sample_size = 10, seq_len = 50):\n",
    "    \n",
    "    char_len = sample_size * seq_len\n",
    "    num_batches = int(len(en_text) / char_len)\n",
    "    \n",
    "    en_text = en_text[: num_batches * char_len]\n",
    "    en_text = en_text.reshape((sample_size, -1))\n",
    "    \n",
    "    for n in range(0,en_text.shape[-1], seq_len):\n",
    "        x = en_text[:, n : n + seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, : -1] = x[:, 1:]\n",
    "            y[:, -1] = en_text[:, n + seq_len]\n",
    "        \n",
    "        except:\n",
    "            y[:, : -1] = x[:, 1:]\n",
    "            y[:, -1] = en_text[:, 0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "sample_text = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = gen_batch(sample_text, sample_size = 2, seq_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden = 256, num_layers = 4, drop_prob = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_pob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.all_chars = all_chars\n",
    "        \n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: i for i, char in decoder.items()}\n",
    "        \n",
    "        # Architecture\n",
    "        self.lstm = nn.LSTM(len(all_chars), num_hidden, num_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        drop_out = drop_out.contiguous().view(-1, self.num_hidden)\n",
    "        output = self.fc1(drop_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                  torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(all_chars = all_char, num_hidden = 512, num_layers = 3, drop_prob = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    params.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(params)\n",
    "# have some of params roughly equal to size of text data set to prevent over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "lr = 0.001\n",
    "train_per = 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_ind = int(len(encoded_text) * train_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "test_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7, 22,  0,  7,  7, 14, 68,  3, 31,  7, 38, 67, 28,\n",
       "       68, 64, 37, 83,  7, 32, 68, 64, 67, 83, 55, 68, 64, 37,  7,  9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "t = 0\n",
    "num_char = max(encoded_text) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 25 | val loss 3.203482151031494\n",
      "epoch: 0 | step: 50 | val loss 3.1927154064178467\n",
      "epoch: 0 | step: 75 | val loss 3.1937904357910156\n",
      "epoch: 0 | step: 100 | val loss 3.183349132537842\n",
      "epoch: 0 | step: 125 | val loss 3.0834155082702637\n",
      "epoch: 0 | step: 150 | val loss 2.9948766231536865\n",
      "epoch: 0 | step: 175 | val loss 2.990663528442383\n",
      "epoch: 0 | step: 200 | val loss 2.8331055641174316\n",
      "epoch: 0 | step: 225 | val loss 2.7366182804107666\n",
      "epoch: 0 | step: 250 | val loss 2.6722817420959473\n",
      "epoch: 0 | step: 275 | val loss 2.5732550621032715\n",
      "epoch: 0 | step: 300 | val loss 2.4460690021514893\n",
      "epoch: 0 | step: 325 | val loss 2.361585855484009\n",
      "epoch: 0 | step: 350 | val loss 2.2889010906219482\n",
      "epoch: 0 | step: 375 | val loss 2.2331292629241943\n",
      "epoch: 0 | step: 400 | val loss 2.196660280227661\n",
      "epoch: 0 | step: 425 | val loss 2.1434803009033203\n",
      "epoch: 0 | step: 450 | val loss 2.1132051944732666\n",
      "epoch: 0 | step: 475 | val loss 2.068819761276245\n",
      "epoch: 1 | step: 500 | val loss 2.036888837814331\n",
      "epoch: 1 | step: 525 | val loss 2.00691819190979\n",
      "epoch: 1 | step: 550 | val loss 1.9803396463394165\n",
      "epoch: 1 | step: 575 | val loss 1.9617390632629395\n",
      "epoch: 1 | step: 600 | val loss 1.936345100402832\n",
      "epoch: 1 | step: 625 | val loss 1.9187614917755127\n",
      "epoch: 1 | step: 650 | val loss 1.885371446609497\n",
      "epoch: 1 | step: 675 | val loss 1.8714358806610107\n",
      "epoch: 1 | step: 700 | val loss 1.850589632987976\n",
      "epoch: 1 | step: 725 | val loss 1.8295953273773193\n",
      "epoch: 1 | step: 750 | val loss 1.8111109733581543\n",
      "epoch: 1 | step: 775 | val loss 1.7950609922409058\n",
      "epoch: 1 | step: 800 | val loss 1.779570460319519\n",
      "epoch: 1 | step: 825 | val loss 1.7673290967941284\n",
      "epoch: 1 | step: 850 | val loss 1.7525619268417358\n",
      "epoch: 1 | step: 875 | val loss 1.7357045412063599\n",
      "epoch: 1 | step: 900 | val loss 1.7213634252548218\n",
      "epoch: 1 | step: 925 | val loss 1.7172000408172607\n",
      "epoch: 1 | step: 950 | val loss 1.6954905986785889\n",
      "epoch: 1 | step: 975 | val loss 1.681961178779602\n",
      "epoch: 2 | step: 1000 | val loss 1.6737213134765625\n",
      "epoch: 2 | step: 1025 | val loss 1.664770483970642\n",
      "epoch: 2 | step: 1050 | val loss 1.6530958414077759\n",
      "epoch: 2 | step: 1075 | val loss 1.6441779136657715\n",
      "epoch: 2 | step: 1100 | val loss 1.6368811130523682\n",
      "epoch: 2 | step: 1125 | val loss 1.6220191717147827\n",
      "epoch: 2 | step: 1150 | val loss 1.6190268993377686\n",
      "epoch: 2 | step: 1175 | val loss 1.608397126197815\n",
      "epoch: 2 | step: 1200 | val loss 1.5961601734161377\n",
      "epoch: 2 | step: 1225 | val loss 1.5866470336914062\n",
      "epoch: 2 | step: 1250 | val loss 1.5825575590133667\n",
      "epoch: 2 | step: 1275 | val loss 1.5755045413970947\n",
      "epoch: 2 | step: 1300 | val loss 1.5685402154922485\n",
      "epoch: 2 | step: 1325 | val loss 1.5703628063201904\n",
      "epoch: 2 | step: 1350 | val loss 1.561505913734436\n",
      "epoch: 2 | step: 1375 | val loss 1.5598888397216797\n",
      "epoch: 2 | step: 1400 | val loss 1.5502986907958984\n",
      "epoch: 2 | step: 1425 | val loss 1.549086332321167\n",
      "epoch: 2 | step: 1450 | val loss 1.5391641855239868\n",
      "epoch: 3 | step: 1475 | val loss 1.533116102218628\n",
      "epoch: 3 | step: 1500 | val loss 1.5278544425964355\n",
      "epoch: 3 | step: 1525 | val loss 1.5253119468688965\n",
      "epoch: 3 | step: 1550 | val loss 1.5210870504379272\n",
      "epoch: 3 | step: 1575 | val loss 1.5147582292556763\n",
      "epoch: 3 | step: 1600 | val loss 1.5101702213287354\n",
      "epoch: 3 | step: 1625 | val loss 1.5040045976638794\n",
      "epoch: 3 | step: 1650 | val loss 1.49524986743927\n",
      "epoch: 3 | step: 1675 | val loss 1.494675874710083\n",
      "epoch: 3 | step: 1700 | val loss 1.4874697923660278\n",
      "epoch: 3 | step: 1725 | val loss 1.4830998182296753\n",
      "epoch: 3 | step: 1750 | val loss 1.4824413061141968\n",
      "epoch: 3 | step: 1775 | val loss 1.4771747589111328\n",
      "epoch: 3 | step: 1800 | val loss 1.4745759963989258\n",
      "epoch: 3 | step: 1825 | val loss 1.4680594205856323\n",
      "epoch: 3 | step: 1850 | val loss 1.4723306894302368\n",
      "epoch: 3 | step: 1875 | val loss 1.4703766107559204\n",
      "epoch: 3 | step: 1900 | val loss 1.4673126935958862\n",
      "epoch: 3 | step: 1925 | val loss 1.4599089622497559\n",
      "epoch: 3 | step: 1950 | val loss 1.4575989246368408\n",
      "epoch: 4 | step: 1975 | val loss 1.4554593563079834\n",
      "epoch: 4 | step: 2000 | val loss 1.4600893259048462\n",
      "epoch: 4 | step: 2025 | val loss 1.4447156190872192\n",
      "epoch: 4 | step: 2050 | val loss 1.448931336402893\n",
      "epoch: 4 | step: 2075 | val loss 1.4461177587509155\n",
      "epoch: 4 | step: 2100 | val loss 1.4390299320220947\n",
      "epoch: 4 | step: 2125 | val loss 1.4367321729660034\n",
      "epoch: 4 | step: 2150 | val loss 1.4318783283233643\n",
      "epoch: 4 | step: 2175 | val loss 1.4330133199691772\n",
      "epoch: 4 | step: 2200 | val loss 1.4307483434677124\n",
      "epoch: 4 | step: 2225 | val loss 1.4275773763656616\n",
      "epoch: 4 | step: 2250 | val loss 1.4275013208389282\n",
      "epoch: 4 | step: 2275 | val loss 1.4247195720672607\n",
      "epoch: 4 | step: 2300 | val loss 1.4219659566879272\n",
      "epoch: 4 | step: 2325 | val loss 1.4231040477752686\n",
      "epoch: 4 | step: 2350 | val loss 1.4252017736434937\n",
      "epoch: 4 | step: 2375 | val loss 1.419924259185791\n",
      "epoch: 4 | step: 2400 | val loss 1.4190882444381714\n",
      "epoch: 4 | step: 2425 | val loss 1.4114209413528442\n",
      "epoch: 4 | step: 2450 | val loss 1.4102624654769897\n",
      "epoch: 5 | step: 2475 | val loss 1.4108177423477173\n",
      "epoch: 5 | step: 2500 | val loss 1.4114999771118164\n",
      "epoch: 5 | step: 2525 | val loss 1.4100587368011475\n",
      "epoch: 5 | step: 2550 | val loss 1.4078043699264526\n",
      "epoch: 5 | step: 2575 | val loss 1.4049055576324463\n",
      "epoch: 5 | step: 2600 | val loss 1.403573989868164\n",
      "epoch: 5 | step: 2625 | val loss 1.4004427194595337\n",
      "epoch: 5 | step: 2650 | val loss 1.3955078125\n",
      "epoch: 5 | step: 2675 | val loss 1.39036226272583\n",
      "epoch: 5 | step: 2700 | val loss 1.3887345790863037\n",
      "epoch: 5 | step: 2725 | val loss 1.397493600845337\n",
      "epoch: 5 | step: 2750 | val loss 1.393553614616394\n",
      "epoch: 5 | step: 2775 | val loss 1.3900786638259888\n",
      "epoch: 5 | step: 2800 | val loss 1.3952419757843018\n",
      "epoch: 5 | step: 2825 | val loss 1.3915202617645264\n",
      "epoch: 5 | step: 2850 | val loss 1.3944259881973267\n",
      "epoch: 5 | step: 2875 | val loss 1.3961020708084106\n",
      "epoch: 5 | step: 2900 | val loss 1.388363003730774\n",
      "epoch: 5 | step: 2925 | val loss 1.387183427810669\n",
      "epoch: 6 | step: 2950 | val loss 1.3817511796951294\n",
      "epoch: 6 | step: 2975 | val loss 1.3860352039337158\n",
      "epoch: 6 | step: 3000 | val loss 1.381532907485962\n",
      "epoch: 6 | step: 3025 | val loss 1.384384036064148\n",
      "epoch: 6 | step: 3050 | val loss 1.3796207904815674\n",
      "epoch: 6 | step: 3075 | val loss 1.3750557899475098\n",
      "epoch: 6 | step: 3100 | val loss 1.3797988891601562\n",
      "epoch: 6 | step: 3125 | val loss 1.372141718864441\n",
      "epoch: 6 | step: 3150 | val loss 1.3771721124649048\n",
      "epoch: 6 | step: 3175 | val loss 1.3670181035995483\n",
      "epoch: 6 | step: 3200 | val loss 1.3708887100219727\n",
      "epoch: 6 | step: 3225 | val loss 1.368477702140808\n",
      "epoch: 6 | step: 3250 | val loss 1.365359902381897\n",
      "epoch: 6 | step: 3275 | val loss 1.3645378351211548\n",
      "epoch: 6 | step: 3300 | val loss 1.372271180152893\n",
      "epoch: 6 | step: 3325 | val loss 1.3773283958435059\n",
      "epoch: 6 | step: 3350 | val loss 1.3820654153823853\n",
      "epoch: 6 | step: 3375 | val loss 1.3752795457839966\n",
      "epoch: 6 | step: 3400 | val loss 1.3687630891799927\n",
      "epoch: 6 | step: 3425 | val loss 1.3659902811050415\n",
      "epoch: 7 | step: 3450 | val loss 1.3679102659225464\n",
      "epoch: 7 | step: 3475 | val loss 1.370855689048767\n",
      "epoch: 7 | step: 3500 | val loss 1.361081838607788\n",
      "epoch: 7 | step: 3525 | val loss 1.3574727773666382\n",
      "epoch: 7 | step: 3550 | val loss 1.3595621585845947\n",
      "epoch: 7 | step: 3575 | val loss 1.358634114265442\n",
      "epoch: 7 | step: 3600 | val loss 1.361959457397461\n",
      "epoch: 7 | step: 3625 | val loss 1.3558340072631836\n",
      "epoch: 7 | step: 3650 | val loss 1.3527657985687256\n",
      "epoch: 7 | step: 3675 | val loss 1.354964017868042\n",
      "epoch: 7 | step: 3700 | val loss 1.3560447692871094\n",
      "epoch: 7 | step: 3725 | val loss 1.3527604341506958\n",
      "epoch: 7 | step: 3750 | val loss 1.3543336391448975\n",
      "epoch: 7 | step: 3775 | val loss 1.3586126565933228\n",
      "epoch: 7 | step: 3800 | val loss 1.3578379154205322\n",
      "epoch: 7 | step: 3825 | val loss 1.3570181131362915\n",
      "epoch: 7 | step: 3850 | val loss 1.3538978099822998\n",
      "epoch: 7 | step: 3875 | val loss 1.3501651287078857\n",
      "epoch: 7 | step: 3900 | val loss 1.3475490808486938\n",
      "epoch: 8 | step: 3925 | val loss 1.3617054224014282\n",
      "epoch: 8 | step: 3950 | val loss 1.356122374534607\n",
      "epoch: 8 | step: 3975 | val loss 1.3534955978393555\n",
      "epoch: 8 | step: 4000 | val loss 1.3497785329818726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | step: 4025 | val loss 1.3488202095031738\n",
      "epoch: 8 | step: 4050 | val loss 1.346407413482666\n",
      "epoch: 8 | step: 4075 | val loss 1.34852933883667\n",
      "epoch: 8 | step: 4100 | val loss 1.3438316583633423\n",
      "epoch: 8 | step: 4125 | val loss 1.3467200994491577\n",
      "epoch: 8 | step: 4150 | val loss 1.3416556119918823\n",
      "epoch: 8 | step: 4175 | val loss 1.3414490222930908\n",
      "epoch: 8 | step: 4200 | val loss 1.3484151363372803\n",
      "epoch: 8 | step: 4225 | val loss 1.34339439868927\n",
      "epoch: 8 | step: 4250 | val loss 1.3423597812652588\n",
      "epoch: 8 | step: 4275 | val loss 1.3435819149017334\n",
      "epoch: 8 | step: 4300 | val loss 1.3435900211334229\n",
      "epoch: 8 | step: 4325 | val loss 1.3469713926315308\n",
      "epoch: 8 | step: 4350 | val loss 1.3461576700210571\n",
      "epoch: 8 | step: 4375 | val loss 1.3414428234100342\n",
      "epoch: 8 | step: 4400 | val loss 1.3343620300292969\n",
      "epoch: 9 | step: 4425 | val loss 1.3403033018112183\n",
      "epoch: 9 | step: 4450 | val loss 1.338165044784546\n",
      "epoch: 9 | step: 4475 | val loss 1.339518666267395\n",
      "epoch: 9 | step: 4500 | val loss 1.340774416923523\n",
      "epoch: 9 | step: 4525 | val loss 1.339187741279602\n",
      "epoch: 9 | step: 4550 | val loss 1.340033769607544\n",
      "epoch: 9 | step: 4575 | val loss 1.3385839462280273\n",
      "epoch: 9 | step: 4600 | val loss 1.334677815437317\n",
      "epoch: 9 | step: 4625 | val loss 1.338047742843628\n",
      "epoch: 9 | step: 4650 | val loss 1.330034613609314\n",
      "epoch: 9 | step: 4675 | val loss 1.3331164121627808\n",
      "epoch: 9 | step: 4700 | val loss 1.3361234664916992\n",
      "epoch: 9 | step: 4725 | val loss 1.3285086154937744\n",
      "epoch: 9 | step: 4750 | val loss 1.332369327545166\n",
      "epoch: 9 | step: 4775 | val loss 1.3371648788452148\n",
      "epoch: 9 | step: 4800 | val loss 1.3371437788009644\n",
      "epoch: 9 | step: 4825 | val loss 1.34066903591156\n",
      "epoch: 9 | step: 4850 | val loss 1.3391979932785034\n",
      "epoch: 9 | step: 4875 | val loss 1.3316534757614136\n",
      "epoch: 9 | step: 4900 | val loss 1.3368980884552002\n",
      "epoch: 10 | step: 4925 | val loss 1.3364944458007812\n",
      "epoch: 10 | step: 4950 | val loss 1.3347856998443604\n",
      "epoch: 10 | step: 4975 | val loss 1.3317456245422363\n",
      "epoch: 10 | step: 5000 | val loss 1.3332273960113525\n",
      "epoch: 10 | step: 5025 | val loss 1.3293215036392212\n",
      "epoch: 10 | step: 5050 | val loss 1.3314826488494873\n",
      "epoch: 10 | step: 5075 | val loss 1.3341567516326904\n",
      "epoch: 10 | step: 5100 | val loss 1.3301796913146973\n",
      "epoch: 10 | step: 5125 | val loss 1.3335224390029907\n",
      "epoch: 10 | step: 5150 | val loss 1.3278909921646118\n",
      "epoch: 10 | step: 5175 | val loss 1.3312629461288452\n",
      "epoch: 10 | step: 5200 | val loss 1.3308638334274292\n",
      "epoch: 10 | step: 5225 | val loss 1.3289496898651123\n",
      "epoch: 10 | step: 5250 | val loss 1.333113431930542\n",
      "epoch: 10 | step: 5275 | val loss 1.3319061994552612\n",
      "epoch: 10 | step: 5300 | val loss 1.3357752561569214\n",
      "epoch: 10 | step: 5325 | val loss 1.333472728729248\n",
      "epoch: 10 | step: 5350 | val loss 1.325014591217041\n",
      "epoch: 10 | step: 5375 | val loss 1.3252087831497192\n",
      "epoch: 11 | step: 5400 | val loss 1.3264594078063965\n",
      "epoch: 11 | step: 5425 | val loss 1.3218986988067627\n",
      "epoch: 11 | step: 5450 | val loss 1.3211101293563843\n",
      "epoch: 11 | step: 5475 | val loss 1.3258633613586426\n",
      "epoch: 11 | step: 5500 | val loss 1.3269016742706299\n",
      "epoch: 11 | step: 5525 | val loss 1.3286832571029663\n",
      "epoch: 11 | step: 5550 | val loss 1.3308351039886475\n",
      "epoch: 11 | step: 5575 | val loss 1.3254605531692505\n",
      "epoch: 11 | step: 5600 | val loss 1.321050763130188\n",
      "epoch: 11 | step: 5625 | val loss 1.31990385055542\n",
      "epoch: 11 | step: 5650 | val loss 1.321736216545105\n",
      "epoch: 11 | step: 5675 | val loss 1.3257644176483154\n",
      "epoch: 11 | step: 5700 | val loss 1.3247767686843872\n",
      "epoch: 11 | step: 5725 | val loss 1.3271751403808594\n",
      "epoch: 11 | step: 5750 | val loss 1.332349181175232\n",
      "epoch: 11 | step: 5775 | val loss 1.3318969011306763\n",
      "epoch: 11 | step: 5800 | val loss 1.3335466384887695\n",
      "epoch: 11 | step: 5825 | val loss 1.3364808559417725\n",
      "epoch: 11 | step: 5850 | val loss 1.3319859504699707\n",
      "epoch: 11 | step: 5875 | val loss 1.3284788131713867\n",
      "epoch: 12 | step: 5900 | val loss 1.3276166915893555\n",
      "epoch: 12 | step: 5925 | val loss 1.3270646333694458\n",
      "epoch: 12 | step: 5950 | val loss 1.3293066024780273\n",
      "epoch: 12 | step: 5975 | val loss 1.3257907629013062\n",
      "epoch: 12 | step: 6000 | val loss 1.3199633359909058\n",
      "epoch: 12 | step: 6025 | val loss 1.32515549659729\n",
      "epoch: 12 | step: 6050 | val loss 1.3229248523712158\n",
      "epoch: 12 | step: 6075 | val loss 1.3240255117416382\n",
      "epoch: 12 | step: 6100 | val loss 1.322788953781128\n",
      "epoch: 12 | step: 6125 | val loss 1.3222237825393677\n",
      "epoch: 12 | step: 6150 | val loss 1.3192614316940308\n",
      "epoch: 12 | step: 6175 | val loss 1.3218926191329956\n",
      "epoch: 12 | step: 6200 | val loss 1.3194527626037598\n",
      "epoch: 12 | step: 6225 | val loss 1.324154257774353\n",
      "epoch: 12 | step: 6250 | val loss 1.3288452625274658\n",
      "epoch: 12 | step: 6275 | val loss 1.3264672756195068\n",
      "epoch: 12 | step: 6300 | val loss 1.332939863204956\n",
      "epoch: 12 | step: 6325 | val loss 1.3245339393615723\n",
      "epoch: 12 | step: 6350 | val loss 1.3205243349075317\n",
      "epoch: 13 | step: 6375 | val loss 1.3188480138778687\n",
      "epoch: 13 | step: 6400 | val loss 1.3214925527572632\n",
      "epoch: 13 | step: 6425 | val loss 1.3220444917678833\n",
      "epoch: 13 | step: 6450 | val loss 1.3226925134658813\n",
      "epoch: 13 | step: 6475 | val loss 1.3217811584472656\n",
      "epoch: 13 | step: 6500 | val loss 1.3124505281448364\n",
      "epoch: 13 | step: 6525 | val loss 1.319493293762207\n",
      "epoch: 13 | step: 6550 | val loss 1.320623517036438\n",
      "epoch: 13 | step: 6575 | val loss 1.3194161653518677\n",
      "epoch: 13 | step: 6600 | val loss 1.3157706260681152\n",
      "epoch: 13 | step: 6625 | val loss 1.3196076154708862\n",
      "epoch: 13 | step: 6650 | val loss 1.3206833600997925\n",
      "epoch: 13 | step: 6675 | val loss 1.318462610244751\n",
      "epoch: 13 | step: 6700 | val loss 1.3149243593215942\n",
      "epoch: 13 | step: 6725 | val loss 1.3254196643829346\n",
      "epoch: 13 | step: 6750 | val loss 1.3195815086364746\n",
      "epoch: 13 | step: 6775 | val loss 1.3238357305526733\n",
      "epoch: 13 | step: 6800 | val loss 1.3218963146209717\n",
      "epoch: 13 | step: 6825 | val loss 1.3218897581100464\n",
      "epoch: 13 | step: 6850 | val loss 1.318941354751587\n",
      "epoch: 14 | step: 6875 | val loss 1.3164867162704468\n",
      "epoch: 14 | step: 6900 | val loss 1.3164072036743164\n",
      "epoch: 14 | step: 6925 | val loss 1.3203922510147095\n",
      "epoch: 14 | step: 6950 | val loss 1.3221702575683594\n",
      "epoch: 14 | step: 6975 | val loss 1.3217705488204956\n",
      "epoch: 14 | step: 7000 | val loss 1.3197649717330933\n",
      "epoch: 14 | step: 7025 | val loss 1.3234747648239136\n",
      "epoch: 14 | step: 7050 | val loss 1.3170888423919678\n",
      "epoch: 14 | step: 7075 | val loss 1.3226239681243896\n",
      "epoch: 14 | step: 7100 | val loss 1.3156187534332275\n",
      "epoch: 14 | step: 7125 | val loss 1.3199481964111328\n",
      "epoch: 14 | step: 7150 | val loss 1.3201857805252075\n",
      "epoch: 14 | step: 7175 | val loss 1.3159258365631104\n",
      "epoch: 14 | step: 7200 | val loss 1.3167684078216553\n",
      "epoch: 14 | step: 7225 | val loss 1.3215274810791016\n",
      "epoch: 14 | step: 7250 | val loss 1.321069359779358\n",
      "epoch: 14 | step: 7275 | val loss 1.3235681056976318\n",
      "epoch: 14 | step: 7300 | val loss 1.3264018297195435\n",
      "epoch: 14 | step: 7325 | val loss 1.3202669620513916\n",
      "epoch: 14 | step: 7350 | val loss 1.3179774284362793\n",
      "epoch: 15 | step: 7375 | val loss 1.3170968294143677\n",
      "epoch: 15 | step: 7400 | val loss 1.3158063888549805\n",
      "epoch: 15 | step: 7425 | val loss 1.319981336593628\n",
      "epoch: 15 | step: 7450 | val loss 1.3187507390975952\n",
      "epoch: 15 | step: 7475 | val loss 1.3140039443969727\n",
      "epoch: 15 | step: 7500 | val loss 1.3181506395339966\n",
      "epoch: 15 | step: 7525 | val loss 1.31627357006073\n",
      "epoch: 15 | step: 7550 | val loss 1.3141286373138428\n",
      "epoch: 15 | step: 7575 | val loss 1.3128734827041626\n",
      "epoch: 15 | step: 7600 | val loss 1.3150554895401\n",
      "epoch: 15 | step: 7625 | val loss 1.313993215560913\n",
      "epoch: 15 | step: 7650 | val loss 1.3163979053497314\n",
      "epoch: 15 | step: 7675 | val loss 1.312442421913147\n",
      "epoch: 15 | step: 7700 | val loss 1.3195420503616333\n",
      "epoch: 15 | step: 7725 | val loss 1.3206900358200073\n",
      "epoch: 15 | step: 7750 | val loss 1.3202202320098877\n",
      "epoch: 15 | step: 7775 | val loss 1.3225558996200562\n",
      "epoch: 15 | step: 7800 | val loss 1.3146618604660034\n",
      "epoch: 15 | step: 7825 | val loss 1.313827395439148\n",
      "epoch: 16 | step: 7850 | val loss 1.316865086555481\n",
      "epoch: 16 | step: 7875 | val loss 1.3159583806991577\n",
      "epoch: 16 | step: 7900 | val loss 1.3168452978134155\n",
      "epoch: 16 | step: 7925 | val loss 1.3155535459518433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | step: 7950 | val loss 1.317154049873352\n",
      "epoch: 16 | step: 7975 | val loss 1.3117581605911255\n",
      "epoch: 16 | step: 8000 | val loss 1.316208839416504\n",
      "epoch: 16 | step: 8025 | val loss 1.3148536682128906\n",
      "epoch: 16 | step: 8050 | val loss 1.3142485618591309\n",
      "epoch: 16 | step: 8075 | val loss 1.3082797527313232\n",
      "epoch: 16 | step: 8100 | val loss 1.3092001676559448\n",
      "epoch: 16 | step: 8125 | val loss 1.311583161354065\n",
      "epoch: 16 | step: 8150 | val loss 1.3118605613708496\n",
      "epoch: 16 | step: 8175 | val loss 1.3058611154556274\n",
      "epoch: 16 | step: 8200 | val loss 1.3160910606384277\n",
      "epoch: 16 | step: 8225 | val loss 1.3135119676589966\n",
      "epoch: 16 | step: 8250 | val loss 1.3198447227478027\n",
      "epoch: 16 | step: 8275 | val loss 1.3144415616989136\n",
      "epoch: 16 | step: 8300 | val loss 1.3120951652526855\n",
      "epoch: 16 | step: 8325 | val loss 1.3125619888305664\n",
      "epoch: 17 | step: 8350 | val loss 1.3114821910858154\n",
      "epoch: 17 | step: 8375 | val loss 1.3085685968399048\n",
      "epoch: 17 | step: 8400 | val loss 1.3151229619979858\n",
      "epoch: 17 | step: 8425 | val loss 1.3102922439575195\n",
      "epoch: 17 | step: 8450 | val loss 1.310790777206421\n",
      "epoch: 17 | step: 8475 | val loss 1.3089118003845215\n",
      "epoch: 17 | step: 8500 | val loss 1.308523178100586\n",
      "epoch: 17 | step: 8525 | val loss 1.308265209197998\n",
      "epoch: 17 | step: 8550 | val loss 1.3090704679489136\n",
      "epoch: 17 | step: 8575 | val loss 1.3032481670379639\n",
      "epoch: 17 | step: 8600 | val loss 1.310499906539917\n",
      "epoch: 17 | step: 8625 | val loss 1.3091368675231934\n",
      "epoch: 17 | step: 8650 | val loss 1.3042393922805786\n",
      "epoch: 17 | step: 8675 | val loss 1.309828281402588\n",
      "epoch: 17 | step: 8700 | val loss 1.3113399744033813\n",
      "epoch: 17 | step: 8725 | val loss 1.3106046915054321\n",
      "epoch: 17 | step: 8750 | val loss 1.3137450218200684\n",
      "epoch: 17 | step: 8775 | val loss 1.3123210668563843\n",
      "epoch: 17 | step: 8800 | val loss 1.3069202899932861\n",
      "epoch: 18 | step: 8825 | val loss 1.3066824674606323\n",
      "epoch: 18 | step: 8850 | val loss 1.305178165435791\n",
      "epoch: 18 | step: 8875 | val loss 1.308940052986145\n",
      "epoch: 18 | step: 8900 | val loss 1.310481309890747\n",
      "epoch: 18 | step: 8925 | val loss 1.3104755878448486\n",
      "epoch: 18 | step: 8950 | val loss 1.2995116710662842\n",
      "epoch: 18 | step: 8975 | val loss 1.3070108890533447\n",
      "epoch: 18 | step: 9000 | val loss 1.3085469007492065\n",
      "epoch: 18 | step: 9025 | val loss 1.3069031238555908\n",
      "epoch: 18 | step: 9050 | val loss 1.3015605211257935\n",
      "epoch: 18 | step: 9075 | val loss 1.3041527271270752\n",
      "epoch: 18 | step: 9100 | val loss 1.3067084550857544\n",
      "epoch: 18 | step: 9125 | val loss 1.3093457221984863\n",
      "epoch: 18 | step: 9150 | val loss 1.3049572706222534\n",
      "epoch: 18 | step: 9175 | val loss 1.3117600679397583\n",
      "epoch: 18 | step: 9200 | val loss 1.3102061748504639\n",
      "epoch: 18 | step: 9225 | val loss 1.3082834482192993\n",
      "epoch: 18 | step: 9250 | val loss 1.3157939910888672\n",
      "epoch: 18 | step: 9275 | val loss 1.3090578317642212\n",
      "epoch: 18 | step: 9300 | val loss 1.306457281112671\n",
      "epoch: 19 | step: 9325 | val loss 1.307446837425232\n",
      "epoch: 19 | step: 9350 | val loss 1.3079403638839722\n",
      "epoch: 19 | step: 9375 | val loss 1.3069499731063843\n",
      "epoch: 19 | step: 9400 | val loss 1.3089995384216309\n",
      "epoch: 19 | step: 9425 | val loss 1.3086611032485962\n",
      "epoch: 19 | step: 9450 | val loss 1.3092247247695923\n",
      "epoch: 19 | step: 9475 | val loss 1.3090547323226929\n",
      "epoch: 19 | step: 9500 | val loss 1.3061530590057373\n",
      "epoch: 19 | step: 9525 | val loss 1.304269552230835\n",
      "epoch: 19 | step: 9550 | val loss 1.302733302116394\n",
      "epoch: 19 | step: 9575 | val loss 1.3026278018951416\n",
      "epoch: 19 | step: 9600 | val loss 1.307752251625061\n",
      "epoch: 19 | step: 9625 | val loss 1.3070173263549805\n",
      "epoch: 19 | step: 9650 | val loss 1.309646725654602\n",
      "epoch: 19 | step: 9675 | val loss 1.3087425231933594\n",
      "epoch: 19 | step: 9700 | val loss 1.3113670349121094\n",
      "epoch: 19 | step: 9725 | val loss 1.312319040298462\n",
      "epoch: 19 | step: 9750 | val loss 1.3100967407226562\n",
      "epoch: 19 | step: 9775 | val loss 1.31235671043396\n",
      "epoch: 19 | step: 9800 | val loss 1.3030966520309448\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in gen_batch(train_data, batch_size, seq_len):\n",
    "        t += 1\n",
    "        \n",
    "        x = one_hot_enc(x, num_char)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_out, hidden = model.forward(inputs, hidden)\n",
    "        \n",
    "        loss = criterion(lstm_out, targets.view(batch_size * seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if t % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in gen_batch(test_data, batch_size, seq_len):\n",
    "                \n",
    "                x = one_hot_enc(x, num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "        \n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_out,val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_out, targets.view(batch_size * seq_len).long())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            print(f\"epoch: {i} | step: {t} | val loss {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converges around a 1.30 loss\n",
    "# premise: loss does not reduce after a whole epoch\n",
    "# save model\n",
    "name = 'CharRNN_hidden512_layers3_shakes.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction \n",
    "def predict_next(model, char, hidden = None, k = 1):\n",
    "    \n",
    "    encoded_text = model.encoder[char]\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    encoded_text = one_hot_enc(encoded_text, len(model.all_chars))\n",
    "    \n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    inputs = inputs.cuda()\n",
    "    \n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "    \n",
    "    probs = F.softmax(lstm_out, dim = 1).data\n",
    "    probs = probs.cpu()\n",
    "    \n",
    "    probs, index_pos = probs.topk(k)\n",
    "    index_pos = index_pos.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    probs = probs/probs.sum()\n",
    "    \n",
    "    char = np.random.choice(index_pos, p = probs)\n",
    "    \n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, size, seed = 'The', k = 1):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
