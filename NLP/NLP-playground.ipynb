{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Implmentation for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/shakespeare.txt', 'r', encoding = 'utf8') as t:\n",
    "    text = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_char = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder = dict(enumerate(all_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = {char: i for i,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 77, 24, 38, 38, 45, 79, 21,  0, 38, 68, 36, 50,\n",
       "       79, 72, 59, 42, 38,  8, 79, 72, 36, 42, 74, 79, 72, 59, 38, 58, 72,\n",
       "       38, 78, 72, 59, 50, 79, 72, 38, 50, 17,  8, 79, 72, 36, 59, 72, 47,\n",
       "       24, 38, 38, 51, 64, 36, 42, 38, 42, 64, 72, 79, 72, 16, 80, 38, 16,\n",
       "       72, 36, 74, 42, 80, 11, 59, 38, 79, 21, 59, 72, 38,  0, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def one_hot_enc(batch_text, uni_chars):\n",
    "    one_hot = np.zeros((batch_text.size, uni_chars))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), batch_text.flatten()] = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*batch_text.shape, uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "x = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_enc(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batches for training\n",
    "def gen_batch(en_text, sample_size = 10, seq_len = 50):\n",
    "    \n",
    "    char_len = sample_size * seq_len\n",
    "    num_batches = int(len(en_text) / char_len)\n",
    "    \n",
    "    en_text = en_text[: num_batches * char_len]\n",
    "    en_text = en_text.reshape((sample_size, -1))\n",
    "    \n",
    "    for n in range(0,en_text.shape[-1], seq_len):\n",
    "        x = en_text[:, n : n + seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, : -1] = x[:, 1:]\n",
    "            y[:, -1] = en_text[:, n + seq_len]\n",
    "        \n",
    "        except:\n",
    "            y[:, : -1] = x[:, 1:]\n",
    "            y[:, -1] = en_text[:, 0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "sample_text = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = gen_batch(sample_text, sample_size = 2, seq_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden = 256, num_layers = 4, drop_prob = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_pob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.all_chars = all_chars\n",
    "        \n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: i for i, char in decoder.items()}\n",
    "        \n",
    "        # Architecture\n",
    "        self.lstm = nn.LSTM(len(all_chars), num_hidden, num_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        drop_out = drop_out.contiguous().view(-1, self.num_hidden)\n",
    "        output = self.fc1(drop_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                  torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(all_chars = all_char, num_hidden = 512, num_layers = 3, drop_prob = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    params.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(params)\n",
    "# have some of params roughly equal to size of text data set to prevent over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "lr = 0.001\n",
    "train_per = 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_ind = int(len(encoded_text) * train_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "test_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 77, 24, 38, 38, 45, 79, 21,  0, 38, 68, 36, 50,\n",
       "       79, 72, 59, 42, 38,  8, 79, 72, 36, 42, 74, 79, 72, 59, 38, 58])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "t = 0\n",
    "num_char = max(encoded_text) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 | step: 25 | val loss 3.2135159969329834\n",
      "epoch : 0 | step: 50 | val loss 3.1979482173919678\n",
      "epoch : 0 | step: 75 | val loss 3.1852169036865234\n",
      "epoch : 0 | step: 100 | val loss 3.067037582397461\n",
      "epoch : 0 | step: 125 | val loss 2.9654741287231445\n",
      "epoch : 0 | step: 150 | val loss 2.8195881843566895\n",
      "epoch : 0 | step: 175 | val loss 2.719484329223633\n",
      "epoch : 0 | step: 200 | val loss 2.6193275451660156\n",
      "epoch : 0 | step: 225 | val loss 2.464092493057251\n",
      "epoch : 0 | step: 250 | val loss 2.353654146194458\n",
      "epoch : 0 | step: 275 | val loss 2.2722887992858887\n",
      "epoch : 0 | step: 300 | val loss 2.204639434814453\n",
      "epoch : 0 | step: 325 | val loss 2.154618263244629\n",
      "epoch : 0 | step: 350 | val loss 2.104109287261963\n",
      "epoch : 0 | step: 375 | val loss 2.0644683837890625\n",
      "epoch : 0 | step: 400 | val loss 2.024813175201416\n",
      "epoch : 0 | step: 425 | val loss 2.0004148483276367\n",
      "epoch : 0 | step: 450 | val loss 1.9739222526550293\n",
      "epoch : 0 | step: 475 | val loss 1.9516977071762085\n",
      "epoch : 1 | step: 500 | val loss 1.907690405845642\n",
      "epoch : 1 | step: 525 | val loss 1.8926959037780762\n",
      "epoch : 1 | step: 550 | val loss 1.8699326515197754\n",
      "epoch : 1 | step: 575 | val loss 1.8522021770477295\n",
      "epoch : 1 | step: 600 | val loss 1.8343087434768677\n",
      "epoch : 1 | step: 625 | val loss 1.8170722723007202\n",
      "epoch : 1 | step: 650 | val loss 1.8011348247528076\n",
      "epoch : 1 | step: 675 | val loss 1.7899227142333984\n",
      "epoch : 1 | step: 700 | val loss 1.7693054676055908\n",
      "epoch : 1 | step: 725 | val loss 1.7567912340164185\n",
      "epoch : 1 | step: 750 | val loss 1.7397210597991943\n",
      "epoch : 1 | step: 775 | val loss 1.7252001762390137\n",
      "epoch : 1 | step: 800 | val loss 1.7116365432739258\n",
      "epoch : 1 | step: 825 | val loss 1.704601526260376\n",
      "epoch : 1 | step: 850 | val loss 1.6915949583053589\n",
      "epoch : 1 | step: 875 | val loss 1.677706003189087\n",
      "epoch : 1 | step: 900 | val loss 1.668468713760376\n",
      "epoch : 1 | step: 925 | val loss 1.6537753343582153\n",
      "epoch : 1 | step: 950 | val loss 1.641465663909912\n",
      "epoch : 1 | step: 975 | val loss 1.6302399635314941\n",
      "epoch : 2 | step: 1000 | val loss 1.5984364748001099\n",
      "epoch : 2 | step: 1025 | val loss 1.5935444831848145\n",
      "epoch : 2 | step: 1050 | val loss 1.5856937170028687\n",
      "epoch : 2 | step: 1075 | val loss 1.5795239210128784\n",
      "epoch : 2 | step: 1100 | val loss 1.5726985931396484\n",
      "epoch : 2 | step: 1125 | val loss 1.5672025680541992\n",
      "epoch : 2 | step: 1150 | val loss 1.557767391204834\n",
      "epoch : 2 | step: 1175 | val loss 1.554183840751648\n",
      "epoch : 2 | step: 1200 | val loss 1.5433151721954346\n",
      "epoch : 2 | step: 1225 | val loss 1.5409342050552368\n",
      "epoch : 2 | step: 1250 | val loss 1.5323354005813599\n",
      "epoch : 2 | step: 1275 | val loss 1.5252974033355713\n",
      "epoch : 2 | step: 1300 | val loss 1.5189603567123413\n",
      "epoch : 2 | step: 1325 | val loss 1.5152347087860107\n",
      "epoch : 2 | step: 1350 | val loss 1.5063307285308838\n",
      "epoch : 2 | step: 1375 | val loss 1.4973686933517456\n",
      "epoch : 2 | step: 1400 | val loss 1.4951367378234863\n",
      "epoch : 2 | step: 1425 | val loss 1.4862464666366577\n",
      "epoch : 2 | step: 1450 | val loss 1.4791325330734253\n",
      "epoch : 3 | step: 1475 | val loss 1.4580610990524292\n",
      "epoch : 3 | step: 1500 | val loss 1.446290135383606\n",
      "epoch : 3 | step: 1525 | val loss 1.449280858039856\n",
      "epoch : 3 | step: 1550 | val loss 1.4459315538406372\n",
      "epoch : 3 | step: 1575 | val loss 1.448256254196167\n",
      "epoch : 3 | step: 1600 | val loss 1.4452840089797974\n",
      "epoch : 3 | step: 1625 | val loss 1.4406486749649048\n",
      "epoch : 3 | step: 1650 | val loss 1.4344183206558228\n",
      "epoch : 3 | step: 1675 | val loss 1.4407627582550049\n",
      "epoch : 3 | step: 1700 | val loss 1.432041049003601\n",
      "epoch : 3 | step: 1725 | val loss 1.424897313117981\n",
      "epoch : 3 | step: 1750 | val loss 1.4247779846191406\n",
      "epoch : 3 | step: 1775 | val loss 1.423195719718933\n",
      "epoch : 3 | step: 1800 | val loss 1.4194397926330566\n",
      "epoch : 3 | step: 1825 | val loss 1.4100836515426636\n",
      "epoch : 3 | step: 1850 | val loss 1.416810393333435\n",
      "epoch : 3 | step: 1875 | val loss 1.412247896194458\n",
      "epoch : 3 | step: 1900 | val loss 1.4081515073776245\n",
      "epoch : 3 | step: 1925 | val loss 1.3997844457626343\n",
      "epoch : 3 | step: 1950 | val loss 1.394517183303833\n",
      "epoch : 4 | step: 1975 | val loss 1.3665013313293457\n",
      "epoch : 4 | step: 2000 | val loss 1.3699419498443604\n",
      "epoch : 4 | step: 2025 | val loss 1.3708012104034424\n",
      "epoch : 4 | step: 2050 | val loss 1.3706029653549194\n",
      "epoch : 4 | step: 2075 | val loss 1.3703107833862305\n",
      "epoch : 4 | step: 2100 | val loss 1.3681319952011108\n",
      "epoch : 4 | step: 2125 | val loss 1.3673267364501953\n",
      "epoch : 4 | step: 2150 | val loss 1.3656407594680786\n",
      "epoch : 4 | step: 2175 | val loss 1.3702256679534912\n",
      "epoch : 4 | step: 2200 | val loss 1.362195372581482\n",
      "epoch : 4 | step: 2225 | val loss 1.365326166152954\n",
      "epoch : 4 | step: 2250 | val loss 1.3609249591827393\n",
      "epoch : 4 | step: 2275 | val loss 1.3610343933105469\n",
      "epoch : 4 | step: 2300 | val loss 1.3566805124282837\n",
      "epoch : 4 | step: 2325 | val loss 1.3546394109725952\n",
      "epoch : 4 | step: 2350 | val loss 1.3547730445861816\n",
      "epoch : 4 | step: 2375 | val loss 1.3547072410583496\n",
      "epoch : 4 | step: 2400 | val loss 1.3506828546524048\n",
      "epoch : 4 | step: 2425 | val loss 1.345808744430542\n",
      "epoch : 4 | step: 2450 | val loss 1.335386037826538\n",
      "epoch : 5 | step: 2475 | val loss 1.3139969110488892\n",
      "epoch : 5 | step: 2500 | val loss 1.3213287591934204\n",
      "epoch : 5 | step: 2525 | val loss 1.320497989654541\n",
      "epoch : 5 | step: 2550 | val loss 1.3241256475448608\n",
      "epoch : 5 | step: 2575 | val loss 1.3252403736114502\n",
      "epoch : 5 | step: 2600 | val loss 1.3246122598648071\n",
      "epoch : 5 | step: 2625 | val loss 1.323500156402588\n",
      "epoch : 5 | step: 2650 | val loss 1.3242559432983398\n",
      "epoch : 5 | step: 2675 | val loss 1.3183531761169434\n",
      "epoch : 5 | step: 2700 | val loss 1.3165807723999023\n",
      "epoch : 5 | step: 2725 | val loss 1.3196550607681274\n",
      "epoch : 5 | step: 2750 | val loss 1.3248441219329834\n",
      "epoch : 5 | step: 2775 | val loss 1.3214685916900635\n",
      "epoch : 5 | step: 2800 | val loss 1.3203808069229126\n",
      "epoch : 5 | step: 2825 | val loss 1.318070411682129\n",
      "epoch : 5 | step: 2850 | val loss 1.3146499395370483\n",
      "epoch : 5 | step: 2875 | val loss 1.3155633211135864\n",
      "epoch : 5 | step: 2900 | val loss 1.3125967979431152\n",
      "epoch : 5 | step: 2925 | val loss 1.3083531856536865\n",
      "epoch : 6 | step: 2950 | val loss 1.277298092842102\n",
      "epoch : 6 | step: 2975 | val loss 1.287937045097351\n",
      "epoch : 6 | step: 3000 | val loss 1.2858842611312866\n",
      "epoch : 6 | step: 3025 | val loss 1.2905073165893555\n",
      "epoch : 6 | step: 3050 | val loss 1.2936089038848877\n",
      "epoch : 6 | step: 3075 | val loss 1.2940691709518433\n",
      "epoch : 6 | step: 3100 | val loss 1.2939506769180298\n",
      "epoch : 6 | step: 3125 | val loss 1.2921799421310425\n",
      "epoch : 6 | step: 3150 | val loss 1.296268105506897\n",
      "epoch : 6 | step: 3175 | val loss 1.2899572849273682\n",
      "epoch : 6 | step: 3200 | val loss 1.292275071144104\n",
      "epoch : 6 | step: 3225 | val loss 1.2892446517944336\n",
      "epoch : 6 | step: 3250 | val loss 1.2946617603302002\n",
      "epoch : 6 | step: 3275 | val loss 1.2915091514587402\n",
      "epoch : 6 | step: 3300 | val loss 1.2882496118545532\n",
      "epoch : 6 | step: 3325 | val loss 1.294709324836731\n",
      "epoch : 6 | step: 3350 | val loss 1.291245698928833\n",
      "epoch : 6 | step: 3375 | val loss 1.2860811948776245\n",
      "epoch : 6 | step: 3400 | val loss 1.2893933057785034\n",
      "epoch : 6 | step: 3425 | val loss 1.3055135011672974\n",
      "epoch : 7 | step: 3450 | val loss 1.264023780822754\n",
      "epoch : 7 | step: 3475 | val loss 1.2631871700286865\n",
      "epoch : 7 | step: 3500 | val loss 1.2683888673782349\n",
      "epoch : 7 | step: 3525 | val loss 1.268844485282898\n",
      "epoch : 7 | step: 3550 | val loss 1.2681130170822144\n",
      "epoch : 7 | step: 3575 | val loss 1.278959035873413\n",
      "epoch : 7 | step: 3600 | val loss 1.269647479057312\n",
      "epoch : 7 | step: 3625 | val loss 1.270316243171692\n",
      "epoch : 7 | step: 3650 | val loss 1.2701358795166016\n",
      "epoch : 7 | step: 3675 | val loss 1.2730931043624878\n",
      "epoch : 7 | step: 3700 | val loss 1.270634651184082\n",
      "epoch : 7 | step: 3725 | val loss 1.2679544687271118\n",
      "epoch : 7 | step: 3750 | val loss 1.2701843976974487\n",
      "epoch : 7 | step: 3775 | val loss 1.270431399345398\n",
      "epoch : 7 | step: 3800 | val loss 1.2675460577011108\n",
      "epoch : 7 | step: 3825 | val loss 1.271815299987793\n",
      "epoch : 7 | step: 3850 | val loss 1.2715684175491333\n",
      "epoch : 7 | step: 3875 | val loss 1.2652932405471802\n",
      "epoch : 7 | step: 3900 | val loss 1.2641600370407104\n",
      "epoch : 8 | step: 3925 | val loss 1.2506754398345947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 8 | step: 3950 | val loss 1.24131441116333\n",
      "epoch : 8 | step: 3975 | val loss 1.2407772541046143\n",
      "epoch : 8 | step: 4000 | val loss 1.2465368509292603\n",
      "epoch : 8 | step: 4025 | val loss 1.251021146774292\n",
      "epoch : 8 | step: 4050 | val loss 1.254417896270752\n",
      "epoch : 8 | step: 4075 | val loss 1.251602292060852\n",
      "epoch : 8 | step: 4100 | val loss 1.2540996074676514\n",
      "epoch : 8 | step: 4125 | val loss 1.251752495765686\n",
      "epoch : 8 | step: 4150 | val loss 1.2541669607162476\n",
      "epoch : 8 | step: 4175 | val loss 1.251842737197876\n",
      "epoch : 8 | step: 4200 | val loss 1.2560356855392456\n",
      "epoch : 8 | step: 4225 | val loss 1.2550889253616333\n",
      "epoch : 8 | step: 4250 | val loss 1.2542452812194824\n",
      "epoch : 8 | step: 4275 | val loss 1.2521226406097412\n",
      "epoch : 8 | step: 4300 | val loss 1.2560330629348755\n",
      "epoch : 8 | step: 4325 | val loss 1.2524782419204712\n",
      "epoch : 8 | step: 4350 | val loss 1.2523118257522583\n",
      "epoch : 8 | step: 4375 | val loss 1.2511502504348755\n",
      "epoch : 8 | step: 4400 | val loss 1.2470382452011108\n",
      "epoch : 9 | step: 4425 | val loss 1.2191017866134644\n",
      "epoch : 9 | step: 4450 | val loss 1.22743821144104\n",
      "epoch : 9 | step: 4475 | val loss 1.228933334350586\n",
      "epoch : 9 | step: 4500 | val loss 1.2290499210357666\n",
      "epoch : 9 | step: 4525 | val loss 1.2389073371887207\n",
      "epoch : 9 | step: 4550 | val loss 1.2359167337417603\n",
      "epoch : 9 | step: 4575 | val loss 1.2368022203445435\n",
      "epoch : 9 | step: 4600 | val loss 1.2382432222366333\n",
      "epoch : 9 | step: 4625 | val loss 1.2407100200653076\n",
      "epoch : 9 | step: 4650 | val loss 1.2385683059692383\n",
      "epoch : 9 | step: 4675 | val loss 1.239529013633728\n",
      "epoch : 9 | step: 4700 | val loss 1.2410603761672974\n",
      "epoch : 9 | step: 4725 | val loss 1.2409933805465698\n",
      "epoch : 9 | step: 4750 | val loss 1.2396506071090698\n",
      "epoch : 9 | step: 4775 | val loss 1.2408349514007568\n",
      "epoch : 9 | step: 4800 | val loss 1.2389229536056519\n",
      "epoch : 9 | step: 4825 | val loss 1.2405414581298828\n",
      "epoch : 9 | step: 4850 | val loss 1.2390978336334229\n",
      "epoch : 9 | step: 4875 | val loss 1.237365961074829\n",
      "epoch : 9 | step: 4900 | val loss 1.2295894622802734\n",
      "epoch : 10 | step: 4925 | val loss 1.207517385482788\n",
      "epoch : 10 | step: 4950 | val loss 1.2142677307128906\n",
      "epoch : 10 | step: 4975 | val loss 1.2159438133239746\n",
      "epoch : 10 | step: 5000 | val loss 1.2237690687179565\n",
      "epoch : 10 | step: 5025 | val loss 1.2247505187988281\n",
      "epoch : 10 | step: 5050 | val loss 1.225007176399231\n",
      "epoch : 10 | step: 5075 | val loss 1.2239452600479126\n",
      "epoch : 10 | step: 5100 | val loss 1.2245657444000244\n",
      "epoch : 10 | step: 5125 | val loss 1.2244688272476196\n",
      "epoch : 10 | step: 5150 | val loss 1.226387858390808\n",
      "epoch : 10 | step: 5175 | val loss 1.2272536754608154\n",
      "epoch : 10 | step: 5200 | val loss 1.2278822660446167\n",
      "epoch : 10 | step: 5225 | val loss 1.2278274297714233\n",
      "epoch : 10 | step: 5250 | val loss 1.2276462316513062\n",
      "epoch : 10 | step: 5275 | val loss 1.2261879444122314\n",
      "epoch : 10 | step: 5300 | val loss 1.2273606061935425\n",
      "epoch : 10 | step: 5325 | val loss 1.2284249067306519\n",
      "epoch : 10 | step: 5350 | val loss 1.2259175777435303\n",
      "epoch : 10 | step: 5375 | val loss 1.2211233377456665\n",
      "epoch : 11 | step: 5400 | val loss 1.201697587966919\n",
      "epoch : 11 | step: 5425 | val loss 1.2005081176757812\n",
      "epoch : 11 | step: 5450 | val loss 1.20828378200531\n",
      "epoch : 11 | step: 5475 | val loss 1.2087129354476929\n",
      "epoch : 11 | step: 5500 | val loss 1.2122976779937744\n",
      "epoch : 11 | step: 5525 | val loss 1.2125903367996216\n",
      "epoch : 11 | step: 5550 | val loss 1.2128897905349731\n",
      "epoch : 11 | step: 5575 | val loss 1.2155494689941406\n",
      "epoch : 11 | step: 5600 | val loss 1.2155344486236572\n",
      "epoch : 11 | step: 5625 | val loss 1.2204248905181885\n",
      "epoch : 11 | step: 5650 | val loss 1.2181109189987183\n",
      "epoch : 11 | step: 5675 | val loss 1.2183301448822021\n",
      "epoch : 11 | step: 5700 | val loss 1.2196381092071533\n",
      "epoch : 11 | step: 5725 | val loss 1.2195521593093872\n",
      "epoch : 11 | step: 5750 | val loss 1.2194640636444092\n",
      "epoch : 11 | step: 5775 | val loss 1.2203452587127686\n",
      "epoch : 11 | step: 5800 | val loss 1.2205170392990112\n",
      "epoch : 11 | step: 5825 | val loss 1.2231228351593018\n",
      "epoch : 11 | step: 5850 | val loss 1.2207306623458862\n",
      "epoch : 11 | step: 5875 | val loss 1.2133278846740723\n",
      "epoch : 12 | step: 5900 | val loss 1.187880277633667\n",
      "epoch : 12 | step: 5925 | val loss 1.1956093311309814\n",
      "epoch : 12 | step: 5950 | val loss 1.2022323608398438\n",
      "epoch : 12 | step: 5975 | val loss 1.1963590383529663\n",
      "epoch : 12 | step: 6000 | val loss 1.1994714736938477\n",
      "epoch : 12 | step: 6025 | val loss 1.2034578323364258\n",
      "epoch : 12 | step: 6050 | val loss 1.202153205871582\n",
      "epoch : 12 | step: 6075 | val loss 1.206691026687622\n",
      "epoch : 12 | step: 6100 | val loss 1.207425594329834\n",
      "epoch : 12 | step: 6125 | val loss 1.2067546844482422\n",
      "epoch : 12 | step: 6150 | val loss 1.2132866382598877\n",
      "epoch : 12 | step: 6175 | val loss 1.2088830471038818\n",
      "epoch : 12 | step: 6200 | val loss 1.208980679512024\n",
      "epoch : 12 | step: 6225 | val loss 1.211866021156311\n",
      "epoch : 12 | step: 6250 | val loss 1.2124416828155518\n",
      "epoch : 12 | step: 6275 | val loss 1.209741234779358\n",
      "epoch : 12 | step: 6300 | val loss 1.214337944984436\n",
      "epoch : 12 | step: 6325 | val loss 1.2108237743377686\n",
      "epoch : 12 | step: 6350 | val loss 1.2077805995941162\n",
      "epoch : 13 | step: 6375 | val loss 1.1864598989486694\n",
      "epoch : 13 | step: 6400 | val loss 1.1836544275283813\n",
      "epoch : 13 | step: 6425 | val loss 1.1848069429397583\n",
      "epoch : 13 | step: 6450 | val loss 1.1893231868743896\n",
      "epoch : 13 | step: 6475 | val loss 1.1953587532043457\n",
      "epoch : 13 | step: 6500 | val loss 1.1924128532409668\n",
      "epoch : 13 | step: 6525 | val loss 1.1961511373519897\n",
      "epoch : 13 | step: 6550 | val loss 1.198386311531067\n",
      "epoch : 13 | step: 6575 | val loss 1.2000283002853394\n",
      "epoch : 13 | step: 6600 | val loss 1.1990337371826172\n",
      "epoch : 13 | step: 6625 | val loss 1.1999255418777466\n",
      "epoch : 13 | step: 6650 | val loss 1.2029671669006348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-1637bbe306a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in gen_batch(train_data, batch_size, seq_len):\n",
    "        t += 1\n",
    "        \n",
    "        x = one_hot_enc(x, num_char)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_out, hidden = model.forward(inputs, hidden)\n",
    "        \n",
    "        loss = criterion(lstm_out, targets.view(batch_size * seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if t % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in gen_batch(test_data, batch_size, seq_len):\n",
    "                \n",
    "                x = one_hot_enc(x, num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "        \n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_out,val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_out, targets.view(batch_size * seq_len).long())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            print(f\"epoch: {i} | step: {t} | val loss {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converges around a X.XX loss\n",
    "# premise: loss does not reduce after a whole epoch\n",
    "# save model\n",
    "name = 'CharRNN_hidden512_layers3_shakes.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
